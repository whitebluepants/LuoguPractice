{\rtf1\ansi\ansicpg936\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fnil\fcharset134 PingFangSC-Regular;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww33400\viewh21000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\tx11964\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \'cc\'dd\'b6\'c8\'cf\'f2\'cf\'c2 BATCHSIZE 8
\f1 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 After 0 training step(s), loss on all data is 5.13118\
After 500 training step(s), loss on all data is 0.429111\
After 1000 training step(s), loss on all data is 0.409789\
After 1500 training step(s), loss on all data is 0.399923\
After 2000 training step(s), loss on all data is 0.394146\
After 2500 training step(s), loss on all data is 0.390597\
\
\
w1:\
[[-0.7000663   0.9136318   0.08953571]\
 [-2.3402493  -0.14641267  0.58823055]]\
w2:\
[[-0.06024267]\
 [ 0.91956186]\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\tx11964\pardirnatural\partightenfactor0
\cf0  [-0.0682071 ]]\
\

\f0 \'cc\'dd\'b6\'c8\'cf\'f2\'cf\'c2
\f1  STEPS 3000->6000\
After 0 training step(s), loss on all data is 5.13118\
After 500 training step(s), loss on all data is 0.429111\
After 1000 training step(s), loss on all data is 0.409789\
After 1500 training step(s), loss on all data is 0.399923\
After 2000 training step(s), loss on all data is 0.394146\
After 2500 training step(s), loss on all data is 0.390597\
After 3000 training step(s), loss on all data is 0.388336\
After 3500 training step(s), loss on all data is 0.386855\
After 4000 training step(s), loss on all data is 0.385863\
After 4500 training step(s), loss on all data is 0.385186\
After 5000 training step(s), loss on all data is 0.384719\
After 5500 training step(s), loss on all data is 0.384391\
\
\
w1:\
[[-0.69458205  0.844646    0.09441539]\
 [-2.3423436  -0.11862345  0.58623695]]\
w2:\
[[-0.07946704]\
 [ 0.8466813 ]\
 [-0.05690449]]\
\

\f0 \'cc\'dd\'b6\'c8\'cf\'f2\'cf\'c2 BATCHSIZE 8->12
\f1 \
After 0 training step(s), loss on all data is 5.14076\
After 500 training step(s), loss on all data is 0.442188\
After 1000 training step(s), loss on all data is 0.417405\
After 1500 training step(s), loss on all data is 0.40572\
After 2000 training step(s), loss on all data is 0.398823\
After 2500 training step(s), loss on all data is 0.394311\
After 3000 training step(s), loss on all data is 0.391474\
After 3500 training step(s), loss on all data is 0.38943\
After 4000 training step(s), loss on all data is 0.388137\
After 4500 training step(s), loss on all data is 0.387108\
After 5000 training step(s), loss on all data is 0.386487\
After 5500 training step(s), loss on all data is 0.385922\
\
\
w1:\
[[-0.696544    0.8598125   0.09352884]\
 [-2.3422253  -0.11885168  0.58622426]]\
w2:\
[[-0.09169023]\
 [ 0.8618274 ]\
 [-0.05526887]]\
\
Momentum
\f0 \'d3\'c5\'bb\'af\'c6\'f7
\f1 \
After 0 training step(s), loss on all data is 5.13118\
After 500 training step(s), loss on all data is 0.384391\
After 1000 training step(s), loss on all data is 0.383592\
After 1500 training step(s), loss on all data is 0.383562\
After 2000 training step(s), loss on all data is 0.383561\
After 2500 training step(s), loss on all data is 0.383561\
After 3000 training step(s), loss on all data is 0.383561\
After 3500 training step(s), loss on all data is 0.383561\
After 4000 training step(s), loss on all data is 0.383561\
After 4500 training step(s), loss on all data is 0.383561\
After 5000 training step(s), loss on all data is 0.383561\
After 5500 training step(s), loss on all data is 0.383561\
\
\
w1:\
[[-0.6130126   0.8313146   0.07557622]\
 [-2.2570562  -0.14489     0.56769776]]\
w2:\
[[-0.10437744]\
 [ 0.7733863 ]\
 [-0.04416781]]\
\
Adam
\f0 \'d3\'c5\'bb\'af\'c6\'f7
\f1 \
After 0 training step(s), loss on all data is 5.20999\
After 500 training step(s), loss on all data is 0.617026\
After 1000 training step(s), loss on all data is 0.392288\
After 1500 training step(s), loss on all data is 0.386432\
After 2000 training step(s), loss on all data is 0.384254\
After 2500 training step(s), loss on all data is 0.383676\
After 3000 training step(s), loss on all data is 0.383573\
After 3500 training step(s), loss on all data is 0.383561\
After 4000 training step(s), loss on all data is 0.383561\
After 4500 training step(s), loss on all data is 0.383561\
After 5000 training step(s), loss on all data is 0.383561\
After 5500 training step(s), loss on all data is 0.383561\
\
\
w1:\
[[-0.3987441   1.0202521   1.0066779 ]\
 [-2.1322303  -0.23811738  1.1239606 ]]\
w2:\
[[-0.44694558]\
 [ 1.0464519 ]\
 [-0.5386719 ]]}